{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"text-align: center;\"><span style=\"color: #333399;\">Lab Course: Distributed Data Analytics</span><br /><span style=\"color: #333399;\">Exercise Sheet 8</span></h2>\n",
    "<h3 style=\"text-align: center;\"><span style=\"color: #333399;\">Syed Wasif Murtaza Jafri- 311226</span></h3>\n",
    "\n",
    "# Exercise 1: Apache Spark Basics\n",
    "## <strong> Part a) Basic Operations on Resilient Distributed Dataset (RDD)\n",
    "<p><strong>Let's have two lists of words as follows:</strong></p>\n",
    "<p><strong>&bull; a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]</strong></p>\n",
    "<p><strong>&bull; b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]</strong></p>\n",
    "<p><strong>Create two RDD objects of a, b and do the following tasks. Words should be remained in the results </strong><strong>of join operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add\n",
    "from pyspark.sql import functions as F\n",
    "from collections import Counter\n",
    "from pyspark.sql.functions import udf\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.functions import mean as _mean, stddev as _stddev, col\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "import dateutil\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import Window\n",
    "from collections import Counter\n",
    "import statistics\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://LAPTOP-37LN6JRK:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# storing session context in variable\n",
    "sc= SparkContext.getOrCreate()\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "local[*] means pyspark will create workers equal to number of cores in pc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 1. Perform rightOuterJoin and fullOuterJoin operations between a and b. Briefly explain your solution.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I initalized two lists a and b. Then I converted both list into list of tuple combining every word in list with it a unique number as rdd join operation requires tuple list.Then I created two RDDs with both tuple list and applied right and inner join operation and then converted back to list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_tuple: [('spark', 1), ('rdd', 2), ('python', 3), ('context', 4), ('create', 5), ('class', 6)]\n",
      "b_tuple: [('operation', 1), ('apache', 2), ('scala', 3), ('lambda', 4), ('parallel', 5), ('partition', 6)]\n"
     ]
    }
   ],
   "source": [
    "a = [\"spark\", \"rdd\", \"python\", \"context\", \"create\", \"class\"]\n",
    "# for rdd converting list to tuple\n",
    "a_tuple = list(zip(a, list(range(1,len(a)+1))))\n",
    "print('a_tuple:',a_tuple)\n",
    "\n",
    "b = [\"operation\", \"apache\", \"scala\", \"lambda\",\"parallel\",\"partition\"]\n",
    "# for rdd converting list to tuple\n",
    "b_tuple = list(zip(b, list(range(1,len(b)+1))))\n",
    "print('b_tuple:',b_tuple)\n",
    "\n",
    "# parallelizing both rdds\n",
    "a_rdd = sc.parallelize(a_tuple)\n",
    "b_rdd =sc.parallelize(b_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Right Outer Join: [('parallel', (None, 5)), ('lambda', (None, 4)), ('scala', (None, 3)), ('operation', (None, 1)), ('apache', (None, 2)), ('partition', (None, 6))]\n",
      "Right Outer Join List: ['parallel', 'lambda', 'scala', 'operation', 'apache', 'partition']\n"
     ]
    }
   ],
   "source": [
    "# taking right outer join of a_rdd with b_rdd\n",
    "right_outer_join= a_rdd.rightOuterJoin(b_rdd)\n",
    "print('Right Outer Join:',right_outer_join.collect())\n",
    "\n",
    "# converting back to list\n",
    "right_outer_join_list = right_outer_join.map(lambda x:x[0])\n",
    "print('Right Outer Join List:',right_outer_join_list.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full Outer Join: [('python', (3, None)), ('spark', (1, None)), ('context', (4, None)), ('create', (5, None)), ('parallel', (None, 5)), ('lambda', (None, 4)), ('class', (6, None)), ('rdd', (2, None)), ('scala', (None, 3)), ('operation', (None, 1)), ('apache', (None, 2)), ('partition', (None, 6))]\n",
      "Full Outer Join List: ['python', 'spark', 'context', 'create', 'parallel', 'lambda', 'class', 'rdd', 'scala', 'operation', 'apache', 'partition']\n"
     ]
    }
   ],
   "source": [
    "# taking full outer join of a_rdd with b_rdd\n",
    "full_outer_join= (a_rdd.fullOuterJoin(b_rdd))\n",
    "print('Full Outer Join:',full_outer_join.collect())\n",
    "\n",
    "# converting back to list\n",
    "full_outer_join_list = full_outer_join.map(lambda x:x[0])\n",
    "print('Full Outer Join List:',full_outer_join_list.collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>2. Using map and reduce functions to count how many times the character \"s\" appears in all a and </strong><strong>b.</strong>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Counting occurance of 's' character in both rdd with help of map function aand then taking union of count Rdds. Reduce sums the counts in final count Rdd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined RDD: ['spark', 'rdd', 'python', 'context', 'create', 'class', 'operation', 'apache', 'scala', 'lambda', 'parallel', 'partition']\n",
      "Count of s in all a and b: [1, 0, 0, 0, 0, 2, 0, 0, 1, 0, 0, 0]\n",
      "Total count of s:  4\n"
     ]
    }
   ],
   "source": [
    "# parallelizing list a and b\n",
    "a_rdd = sc.parallelize(a)\n",
    "b_rdd =sc.parallelize(b)\n",
    "\n",
    "# combining both rdds\n",
    "c_rdd = a_rdd.union(b_rdd)\n",
    "print('Combined RDD:',c_rdd.collect())\n",
    "\n",
    "# counting s character in each word of list a and b\n",
    "count_rdd = c_rdd.map(lambda x:x.count('s'))\n",
    "print('Count of s in all a and b:',count_rdd.collect())\n",
    "\n",
    "# summing counts with reduce \n",
    "total_count = count_rdd.reduce(add)\n",
    "print('Total count of s: ',total_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>3. Using aggregate function to count how many times the character \"s\" appears in all a and b.</strong></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aggregate function take 3 arguments,zeroValue which is initial value for result,seqOp which defines operation you want to apply on Rdd and  combOp which combines results from different partitions.seqOp gives counts for each partition and then combOp is combining results from each partition to give final count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined RDD: ['spark', 'rdd', 'python', 'context', 'create', 'class', 'operation', 'apache', 'scala', 'lambda', 'parallel', 'partition']\n",
      "Count of s with aggregate: 4\n"
     ]
    }
   ],
   "source": [
    "# parallelizing list a and b\n",
    "a_rdd = sc.parallelize(a)\n",
    "b_rdd =sc.parallelize(b)\n",
    "\n",
    "# combining both rdds\n",
    "c_rdd = a_rdd.union(b_rdd)\n",
    "print('Combined RDD:',c_rdd.collect())\n",
    "\n",
    "# defining seqOp argument which calculates count in each partition\n",
    "seqOp = (lambda local_result, list_element: (local_result + list_element.count('s')) )\n",
    "\n",
    "# defining combOp argument which combine count from partition\n",
    "combOp = (lambda some_local_result, another_local_result: (some_local_result + another_local_result) )\n",
    "\n",
    "rdd_c_agg =  rdd_c.aggregate( 0, seqOp, combOp)\n",
    "print('Count of s with aggregate:',rdd_c_agg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## <strong> Part b) Basic Operations on DataFrames\n",
    "<strong>Use dataset students.json (download from learnweb) for this exercise. First creating DataFrames from the dataset and do several tasks as follows:</strong>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|  null|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#reading jason\n",
    "data= sc.textFile('students.json').collect()\n",
    "df = spark.read.json(sc.parallelize(data))\n",
    "df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  <p><strong>1. Replace the null value(s) in column points by the mean of all points.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 11.736842105263158\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|     null|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|              null|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|     null|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<bound method DataFrame.printSchema of DataFrame[course: string, dob: string, first_name: string, last_name: string, points: bigint, s_id: bigint]>"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculating mean\n",
    "meanPoints = df.select(_mean(col('points')).alias('mean')).collect()[0]['mean']\n",
    "print('Mean:',meanPoints)\n",
    "\n",
    "# replacing null values with mean\n",
    "df=df.na.fill(value=meanPoints,subset=[\"points\"])\n",
    "df.show()\n",
    "df.printSchema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>point column is bigint thats why 11.7 is converted to 11."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>2. Replace the null value(s) in column dob and column last name by \"unknown\" and \"--\" respectively.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+----------+---------+------+----+\n",
      "|            course|               dob|first_name|last_name|points|s_id|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "|Humanities and Art|  October 14, 1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|September 26, 1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|     June 12, 1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|     April 5, 1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|  November 1, 1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|  17 February 1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|    1 January 1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|  January 13, 1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|  26 December 1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|  30 December 1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|     June 12, 1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|      July 2, 1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|     July 22, 1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|   7 February 1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|      May 18, 1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|   August 10, 1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|  16 December 1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|           unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|      7 March 1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|      June 2, 1985|   Zachary|       --|    10|  20|\n",
      "+------------------+------------------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# replacing null value in dob colummn with unknown\n",
    "df=df.na.fill(value=\"unknown\",subset=[\"dob\"])\n",
    "\n",
    "# replacing null value in last_name colummn with --\n",
    "df=df.na.fill(value=\"--\",subset=[\"last_name\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>3. In the dob column, there exist several formats of dates, e.g. October 14, 1983 and 26 December 1989. Let's convert all the dates into DD-MM-YYYY format where DD, MM and YYYY are two digits for day, two digits for months and four digits for year respectively.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+\n",
      "|            course|       dob|first_name|last_name|points|s_id|\n",
      "+------------------+----------+----------+---------+------+----+\n",
      "|Humanities and Art|14-10-1983|      Alan|      Joe|    10|   1|\n",
      "|  Computer Science|26-09-1980|    Martin|  Genberg|    17|   2|\n",
      "|    Graphic Design|12-06-1982|     Athur|   Watson|    16|   3|\n",
      "|    Graphic Design|05-04-1987|  Anabelle|  Sanberg|    12|   4|\n",
      "|        Psychology|01-11-1978|      Kira| Schommer|    11|   5|\n",
      "|          Business|17-02-1981| Christian|   Kiriam|    10|   6|\n",
      "|  Machine Learning|01-01-1984|   Barbara|  Ballard|    14|   7|\n",
      "|     Deep Learning|13-01-1978|      John|       --|    10|   8|\n",
      "|  Machine Learning|26-12-1989|    Marcus|   Carson|    15|   9|\n",
      "|           Physics|30-12-1987|     Marta|   Brooks|    11|  10|\n",
      "|    Data Analytics|12-06-1975|     Holly| Schwartz|    12|  11|\n",
      "|  Computer Science|02-07-1985|     April|    Black|    11|  12|\n",
      "|  Computer Science|22-07-1980|     Irene|  Bradley|    13|  13|\n",
      "|        Psychology|07-02-1986|      Mark|    Weber|    12|  14|\n",
      "|       Informatics|18-05-1987|     Rosie|   Norman|     9|  15|\n",
      "|          Business|10-08-1984|    Martin|   Steele|     7|  16|\n",
      "|  Machine Learning|16-12-1990|     Colin| Martinez|     9|  17|\n",
      "|    Data Analytics|   unknown|   Bridget|    Twain|     6|  18|\n",
      "|          Business|07-03-1980|   Darlene|    Mills|    19|  19|\n",
      "|    Data Analytics|02-06-1985|   Zachary|       --|    10|  20|\n",
      "+------------------+----------+----------+---------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a user defined function which change format\n",
    "@udf('string')\n",
    "def changeFormat(date):\n",
    "    if(date!='unknown'):\n",
    "        # converting str to datetime\n",
    "        date = dateutil.parser.parse(date)\n",
    "        # converting datetime to string of required format\n",
    "        formatedDateStr = date.strftime(\"%d-%m-%Y\")\n",
    "        return formatedDateStr\n",
    "    else:\n",
    "        # if unknown value comes it returns unknown\n",
    "        return 'unknown'\n",
    "    \n",
    "# updating dob column with udf\n",
    "df=df.withColumn(\"dob\",changeFormat(df.dob))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>4. Insert a new column age and calculate the current age of all students.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+---+\n",
      "|            course|       dob|first_name|last_name|points|s_id|age|\n",
      "+------------------+----------+----------+---------+------+----+---+\n",
      "|Humanities and Art|14-10-1983|      Alan|      Joe|    10|   1| 38|\n",
      "|  Computer Science|26-09-1980|    Martin|  Genberg|    17|   2| 41|\n",
      "|    Graphic Design|12-06-1982|     Athur|   Watson|    16|   3| 40|\n",
      "|    Graphic Design|05-04-1987|  Anabelle|  Sanberg|    12|   4| 35|\n",
      "|        Psychology|01-11-1978|      Kira| Schommer|    11|   5| 43|\n",
      "|          Business|17-02-1981| Christian|   Kiriam|    10|   6| 41|\n",
      "|  Machine Learning|01-01-1984|   Barbara|  Ballard|    14|   7| 38|\n",
      "|     Deep Learning|13-01-1978|      John|       --|    10|   8| 44|\n",
      "|  Machine Learning|26-12-1989|    Marcus|   Carson|    15|   9| 32|\n",
      "|           Physics|30-12-1987|     Marta|   Brooks|    11|  10| 34|\n",
      "|    Data Analytics|12-06-1975|     Holly| Schwartz|    12|  11| 47|\n",
      "|  Computer Science|02-07-1985|     April|    Black|    11|  12| 37|\n",
      "|  Computer Science|22-07-1980|     Irene|  Bradley|    13|  13| 41|\n",
      "|        Psychology|07-02-1986|      Mark|    Weber|    12|  14| 36|\n",
      "|       Informatics|18-05-1987|     Rosie|   Norman|     9|  15| 35|\n",
      "|          Business|10-08-1984|    Martin|   Steele|     7|  16| 37|\n",
      "|  Machine Learning|16-12-1990|     Colin| Martinez|     9|  17| 31|\n",
      "|    Data Analytics|   unknown|   Bridget|    Twain|     6|  18|  0|\n",
      "|          Business|07-03-1980|   Darlene|    Mills|    19|  19| 42|\n",
      "|    Data Analytics|02-06-1985|   Zachary|       --|    10|  20| 37|\n",
      "+------------------+----------+----------+---------+------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a udf for calculating age from dob for each rows \n",
    "@udf(\"int\")\n",
    "def calculateAge(birthDateStr):\n",
    "    if(birthDateStr!='unknown'):\n",
    "        today = date.today()\n",
    "        # converting string to date\n",
    "        birthDate = datetime.strptime(birthDateStr, '%d-%m-%Y').date()\n",
    "        \n",
    "        # if todays month and date is less than dob month and date then subtracting 1\n",
    "        age = today.year - birthDate.year-((today.month, today.day) <(birthDate.month, birthDate.day))\n",
    "    else:\n",
    "        age = 0\n",
    "    return age\n",
    "# creating age column with udf\n",
    "df=df.withColumn(\"age\",calculateAge(df.dob))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>5. Let's consider granting some points for good performed students in the class. For each student,f his point is larger than 1 standard deviation of all points, then we update his current point to 20, which is the maximum. See Annex 1 for a tutorial on how to calculate standard deviation.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std of points: 3.2460502314756554\n",
      "mean of points: 11.7\n"
     ]
    }
   ],
   "source": [
    "# calculating standard deviation and adding in a column\n",
    "stdPoints = df.select(_stddev(col('points')).alias('std')).collect()[0]['std']\n",
    "print('std of points:',stdPoints)\n",
    "# adding points to df\n",
    "df = df.withColumn(\"std\",F.lit(stdPoints))\n",
    "\n",
    "# calculating mean and adding in a column\n",
    "meanPoints = df.select(_mean(col('points')).alias('mean')).collect()[0]['mean']\n",
    "print('mean of points:',meanPoints)\n",
    "# adding points to df\n",
    "df = df.withColumn(\"mean\",F.lit(meanPoints))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+----------+----------+---------+------+----+---+\n",
      "|            course|       dob|first_name|last_name|points|s_id|age|\n",
      "+------------------+----------+----------+---------+------+----+---+\n",
      "|Humanities and Art|14-10-1983|      Alan|      Joe|    10|   1| 38|\n",
      "|  Computer Science|26-09-1980|    Martin|  Genberg|    20|   2| 41|\n",
      "|    Graphic Design|12-06-1982|     Athur|   Watson|    20|   3| 40|\n",
      "|    Graphic Design|05-04-1987|  Anabelle|  Sanberg|    12|   4| 35|\n",
      "|        Psychology|01-11-1978|      Kira| Schommer|    11|   5| 43|\n",
      "|          Business|17-02-1981| Christian|   Kiriam|    10|   6| 41|\n",
      "|  Machine Learning|01-01-1984|   Barbara|  Ballard|    14|   7| 38|\n",
      "|     Deep Learning|13-01-1978|      John|       --|    10|   8| 44|\n",
      "|  Machine Learning|26-12-1989|    Marcus|   Carson|    20|   9| 32|\n",
      "|           Physics|30-12-1987|     Marta|   Brooks|    11|  10| 34|\n",
      "|    Data Analytics|12-06-1975|     Holly| Schwartz|    12|  11| 47|\n",
      "|  Computer Science|02-07-1985|     April|    Black|    11|  12| 37|\n",
      "|  Computer Science|22-07-1980|     Irene|  Bradley|    13|  13| 41|\n",
      "|        Psychology|07-02-1986|      Mark|    Weber|    12|  14| 36|\n",
      "|       Informatics|18-05-1987|     Rosie|   Norman|     9|  15| 35|\n",
      "|          Business|10-08-1984|    Martin|   Steele|     7|  16| 37|\n",
      "|  Machine Learning|16-12-1990|     Colin| Martinez|     9|  17| 31|\n",
      "|    Data Analytics|   unknown|   Bridget|    Twain|     6|  18|  0|\n",
      "|          Business|07-03-1980|   Darlene|    Mills|    20|  19| 42|\n",
      "|    Data Analytics|02-06-1985|   Zachary|       --|    10|  20| 37|\n",
      "+------------------+----------+----------+---------+------+----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining udf for updating points\n",
    "@udf(\"int\")\n",
    "def updatePoints(points,std,mean):\n",
    "    if points >  mean + std :\n",
    "        points = 20\n",
    "    return points\n",
    "\n",
    "# updating column points and passing 3 columns to udf (points,std,mean)\n",
    "df=df.withColumn(\"points\",updatePoints(df.points,df.std,df.mean))\n",
    "\n",
    "# removing extra columns\n",
    "df=df.drop(\"std\",\"mean\")\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>6. Create a histogram on the new points created in the task 5.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEWCAYAAAB8LwAVAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAAAdF0lEQVR4nO3df7xcdX3n8dfbRJAfCghXV5PUBAh1gwJqCNCHokJXk4pEt0GDWMFli9ViLcV1o7YUse2CWKgu2JUVBEEETIVNlyig6EItYC7Ir8uPeo1IEhCu/I7Ij5D3/nG+ocPk3HsnyT135ibv5+NxH3PO93zPmc9MMvOec87M98g2ERER7V7U7QIiIqI3JSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiep6kAUlv63Yd3STpvZJWSFot6Q1juN3VknYdq+3F5iUBEV0l6R5Jv9/WdpSkf1k3b3tP2z8aZTvTJVnS5IZK7bYvAsfa3t72T9sXlsf+m/KGv0rSaZImjbbRsr3lnRRQ7mP3jag9JqgEREQHeiB4XgMMjNJnb9vbAwcDHwD+uPGqYrOWgIie17qXIWmOpH5Jj0t6QNJppds15fbR8in6AEkvkvSXkn4p6UFJ35C0Q8t2P1SWPSTpr9ru50RJiyVdIOlx4Khy39dJelTS/ZLOkLRVy/Ys6WOSfibpCUmfl7SbpH8t9V7S2r/tMdbWKmlrSauBScAtkn4+2vNl+y7gWuB1Zdt/LGlQ0sOSlkh6dVvNu5fpcyWdKenyUv8NknYry9Y9v7eU5/f9knaR9H/L8/GwpGsl5T1lM5J/zJhovgR8yfbLgN2AS0r7geV2x3LY5DrgqPL3dmBXYHvgDABJs4CvAEcArwJ2AKa03dd8YDGwI/BN4DngOGAX4ACqT+ofa1vnncCbgP2BTwFnAR8EplG9YR8+zOOqrdX202WvAKo9hN2GfWaK8tjeAvxU0kHA/wDeVx7nL4GLRlh9IfA5YCdgEPhbANvrnt+9y/N7MXA8sBLoA14JfAbI2D2bkQRE9ILLyqfQRyU9SvXGPZxngd0l7WJ7te3rR+h7BHCa7eW2VwOfBhaWw0ULgH+2/S+2nwFOYP03t+tsX2Z7re3f2r7R9vW219i+B/gq8Na2db5g+3HbA8DtwJXl/h8DvgsMd4J5pFo7dZOkR4B/Br4GfL1s9xzbN9l+umz3AEnTh9nGpbZ/YnsNVSjuM8L9PUsVOq+x/azta53B3TYrCYjoBe+xveO6P9b/VN7qaGAP4C5JyyQdMkLfV1N9Yl7nl8Bkqk+7rwZWrFtg+0ngobb1V7TOSNqjHFL5VTns9HdUexOtHmiZ/m3N/PbUG6nWTr3R9k62d7P9l7bXtm+3hM9DrL+3tM6vWqafHKFegFOp9jKulLRc0qINqDUmgARETCi2f2b7cOAVwCnAYknbUX9o4z6qk7vr/A6whupN+35g6roFkrYBdm6/u7b5fwTuAmaWQ1yfAbTxj6bjWsdsu+W52hlYtYnbxfYTto+3vStwKPAXkg7e1O1G70hAxIQi6YOS+sqn40dL81pgqNy2fqf/W8BxkmZI2p7qE//F5fDJYuDdkn6vnDg+kdHf7F8KPA6slvRa4KNj9LBGq3VTt/thSftI2rps94ZyiGxDPUDL8yvpEEm7SxLwGNU5mrWbWG/0kARETDRzgYHyzZ4vAQvL+YEnqU6o/ricy9gfOAc4n+obTr8AngI+DlDOEXyc6oTt/cBq4EHg6RHu+5NUXx99AvjfwMVj+LiGrXVT2P4+8FfAP1E9zt2oTkRvjBOB88rz+z5gJvB9qufuOuArtn+4qTVH71DOKUVA+dT+KNXho190uZyInpA9iNhiSXq3pG3LcfkvArcB93S3qojekYCILdl8qpO491EdLlmYr2lG/LscYoqIiFrZg4iIiFrdHoBszOyyyy6ePn16t8uIiJhQbrzxxl/b7qtbttkExPTp0+nv7+92GRERE4qkXw63LIeYIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiajUaEJLmSrq7XA93vYuJSDpQ0k2S1kha0LbsdyRdKelOSXeMcAWsiIhoQGMBIWkScCYwD5gFHF6uldvqXqrr8F5Ys4lvAKfa/o/AHKqhmCMiYpw0+UO5OcCg7eUAki6iGhztjnUd1l20RNILLjJSgmSy7atKv9UN1hkRETWaDIgpvPCaviuB/Tpcdw/gUUnfAWZQXZRkke3nxrbELdv0RZd3u4Rxdc/J7+p2CRETSq+epJ4MvIXqCl77Ul3m8Kj2TpKOkdQvqX9oaGh8K4yI2Mw1GRCrgGkt81Pp/ELpK4GbbS8v1+S9DHhjeyfbZ9mebXt2X1/tWFMREbGRmgyIZcDMchH2raiug7tkA9bdUdK6d/2DaDl3ERERzWssIMon/2OBK4A7gUtsD0g6SdKhAJL2lbQSOAz4qqSBsu5zVIeXfiDpNkBUF4mPiIhx0uhw37aXAkvb2k5omV5Gdeipbt2rgL2arC8iIobXqyepIyKiyxIQERFRKwERERG1EhAREVErAREREbUSEBERUSsBERERtRIQERFRKwERERG1EhAREVErAREREbUSEBERUSsBERERtRIQERFRKwERERG1EhAREVErAREREbUaDQhJcyXdLWlQ0qKa5QdKuknSGkkLapa/TNJKSWc0WWdERKyvsYCQNAk4E5gHzAIOlzSrrdu9wFHAhcNs5vPANU3VGBERw2tyD2IOMGh7ue1ngIuA+a0dbN9j+1ZgbfvKkt4EvBK4ssEaIyJiGE0GxBRgRcv8ytI2KkkvAv4e+OQo/Y6R1C+pf2hoaKMLjYiI9fXqSeqPAUttrxypk+2zbM+2Pbuvr2+cSouI2DJMbnDbq4BpLfNTS1snDgDeIuljwPbAVpJW217vRHdERDSjyYBYBsyUNIMqGBYCH+hkRdtHrJuWdBQwO+EQETG+GjvEZHsNcCxwBXAncIntAUknSToUQNK+klYChwFflTTQVD0REbFhmtyDwPZSYGlb2wkt08uoDj2NtI1zgXMbKC8iIkbQqyepIyKiyxIQERFRKwERERG1EhAREVErAREREbUSEBERUSsBERERtRIQERFRKwERERG1EhAREVErAREREbUSEBERUSsBERERtRIQERFRKwERERG1EhAREVErAREREbUaDQhJcyXdLWlQ0nrXlJZ0oKSbJK2RtKClfR9J10kakHSrpPc3WWdERKyvsYCQNAk4E5gHzAIOlzSrrdu9wFHAhW3tTwIfsr0nMBf4B0k7NlVrRESsr8lrUs8BBm0vB5B0ETAfuGNdB9v3lGVrW1e0/W8t0/dJehDoAx5tsN6IiGjR5CGmKcCKlvmVpW2DSJoDbAX8vGbZMZL6JfUPDQ1tdKEREbG+nj5JLelVwPnAh22vbV9u+yzbs23P7uvrG/8CIyI2Y00GxCpgWsv81NLWEUkvAy4HPmv7+jGuLSIiRtFkQCwDZkqaIWkrYCGwpJMVS/9LgW/YXtxgjRERMYzGAsL2GuBY4ArgTuAS2wOSTpJ0KICkfSWtBA4DvippoKz+PuBA4ChJN5e/fZqqNSIi1tfkt5iwvRRY2tZ2Qsv0MqpDT+3rXQBc0GRtERExsp4+SR0REd2TgIiIiFoJiIiIqJWAiIiIWgmIiIiolYCIiIhaCYiIiKiVgIiIiFoJiIiIqJWAiIiIWgmIiIiolYCIiIhaCYiIiKiVgIiIiFoJiIiIqJWAiIiIWgmIiIio1VFASNpO0ovK9B6SDpX04g7WmyvpbkmDkhbVLD9Q0k2S1kha0LbsSEk/K39HdvqAIiJibHS6B3EN8BJJU4ArgT8Czh1pBUmTgDOBecAs4HBJs9q63QscBVzYtu7Lgb8G9gPmAH8taacOa42IiDHQaUDI9pPAfwa+YvswYM9R1pkDDNpebvsZ4CJgfmsH2/fYvhVY27buO4GrbD9s+xHgKmBuh7VGRMQY6DggJB0AHAFcXtomjbLOFGBFy/zK0taJjtaVdIykfkn9Q0NDHW46IiI60WlAfAL4NHCp7QFJuwI/bK6sztg+y/Zs27P7+vq6XU5ExGZlcof9Xmn70HUztpdLunaUdVYB01rmp5a2TqwC3ta27o86XDciIsZAp3sQn+6wrdUyYKakGZK2AhYCSzq8vyuAd0jaqZycfkdpi4iIcTLiHoSkecAfAFMkfbll0cuANSOta3uNpGOp3tgnAeeUw1MnAf22l0jaF7gU2Al4t6TP2d7T9sOSPk8VMgAn2X54ox5hRERslNEOMd0H9AOHAje2tD8BHDfaxm0vBZa2tZ3QMr2M6vBR3brnAOeMdh8REdGMEQPC9i3ALZIutP3sONUUERE9oNOT1HMknQi8pqwjwLZ3baqwiIjork4D4myqQ0o3As81V05ERPSKTgPiMdvfbbSSiIjoKZ0GxA8lnQp8B3h6XaPtmxqpKiIiuq7TgNiv3M5uaTNw0NiWExERvaKjgLD99qYLiYiI3tJRQEg6oa7d9kljW05ERPSKTg8x/aZl+iXAIcCdY19ORHOmL7p89E6bmXtOfle3S4gJrNNDTH/fOi/pi2RspIiIzdrGXpN6W4YZIiMiIjYPnZ6DuI3qW0tQDbzXB+T8Q0TEZqzTcxCHtEyvAR6wPeJorhERMbF1dIjJ9i+BHYF3A+8FZjVYU0RE9ICOAkLSJ4BvAq8of9+U9PEmC4uIiO7q9BDT0cB+tn8DIOkU4DrgfzZVWEREdFen32ISLxzF9bnSFhERm6lOA+LrwA2STizXhbieagjwEUmaK+luSYOSFtUs31rSxWX5DZKml/YXSzpP0m2S7pQ02vWvIyJijHV6kvo04MPAw+Xvw7b/YaR1JE0CzgTmUZ3UPlxS+8nto4FHbO8OnA6cUtoPA7a2/XrgTcBH1oVHRESMjxEDQtK+kuZBNbS37S/b/jLwKklvGmXbc4BB28ttPwNcBMxv6zMfOK9MLwYOliSq31xsJ2kysA3wDPD4hjywiIjYNKPtQZwC3FHTPgCcOsq6U4AVLfMrS1ttn/K7iseAnanC4jfA/cC9wBdtP9x+B5KOkdQvqX9oaGiUciIiYkOMFhAvLb+BeIHStkszJQHV3sdzwKuBGcDxkta7/rXts2zPtj27r6+vwXIiIrY8owXETiMs23aUdVcB01rmp5a22j7lcNIOwEPAB4Dv2X7W9oPAj3nhxYoiIqJhowXE9yX9bTkvAIAqJwFXj7LuMmCmpBmStgIWAkva+iwBjizTC4CrbZvqsNJB5f62A/YH7urkAUVExNgY7YdyxwNfAwYl3Vza9gb6gf860oq210g6lmpY8EnAObYHSrj0215C9VXZ8yUNUn07amFZ/Uzg65IGqH5v8XXbt27wo4uIiI02YkCUX04fXo7/71maB2wv72TjtpcCS9vaTmiZforqK63t662ua4+IiPHT6QWDlkt6GngNMFXS1NJ+TZPFRURE93R6PYhTgPdTfb11bWk2kICIiNhMdTpY33uA37X9dIO1RERED+l0LKblwIubLCQiInpLp3sQTwI3S/oB8PxehO0/a6SqiIjouk4DYgnr/4YhIqLnTF90ebdLGHf3nPyuRrbb6beYzhu9V0REbE5GDAhJl9h+n6TbqL619AK292qssoiI6KrR9iA+UW4PabqQiIjoLaP9kvr+cvv8iK6SdgEeKmMmRUTEZmq0CwbtL+lHkr4j6Q2SbgduBx6QNHd8SoyIiG4Y7RDTGcBnqIbhvhqYZ/t6Sa8FvgV8r+H6IiKiS0b7odxk21fa/jbwK9vXA9jO0NsREZu50QJibcv0b9uW5RxERMRmbLRDTHtLepzqmgzblGnK/EsarSwiIrpqtG8xTRqvQiIiord0OljfRpE0V9LdkgYlLapZvrWki8vyGyRNb1m2l6TrJA1Iuk1S9lgiIsZRYwEhaRLVpUPnAbOorkw3q63b0cAjtncHTgdOKetOBi4A/sT2nsDbgGebqjUiItbX5B7EHGDQ9nLbzwAXAfPb+swH1o3ztBg4WJKAdwC32r4FwPZDtp9rsNaIiGjTZEBMAVa0zK8sbbV9bK8BHgN2BvYALOkKSTdJ+lTdHUg6RlK/pP6hoaExfwAREVuyRs9BbILJwJuBI8rteyUd3N7J9lm2Z9ue3dfXN941RkRs1poMiFXAtJb5qaWttk8577AD8BDV3sY1tn9t+0lgKfDGBmuNiIg2TQbEMmCmpBmStgIWsv5Fh5YAR5bpBcDVZRDAK4DXS9q2BMdbgTsarDUiItp0ekW5DWZ7jaRjqd7sJwHn2B6QdBLQb3sJcDZwvqRB4GGqEMH2I5JOowoZA0ttb3mXiYqI6KLGAgLA9lKqw0OtbSe0TD8FHDbMuhdQfdU1IiK6oFdPUkdERJclICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqNRoQkuZKulvSoKRFNcu3lnRxWX6DpOlty39H0mpJn2yyzoiIWF9jASFpEnAmMA+YBRwuaVZbt6OBR2zvDpwOnNK2/DTgu03VGBERw2tyD2IOMGh7ue1ngIuA+W195gPnlenFwMGSBCDpPcAvgIEGa4yIiGE0GRBTgBUt8ytLW20f22uAx4CdJW0P/HfgcyPdgaRjJPVL6h8aGhqzwiMiondPUp8InG579UidbJ9le7bt2X19feNTWUTEFmJyg9teBUxrmZ9a2ur6rJQ0GdgBeAjYD1gg6QvAjsBaSU/ZPqPBeiMiokWTAbEMmClpBlUQLAQ+0NZnCXAkcB2wALjatoG3rOsg6URgdcIhImJ8NRYQttdIOha4ApgEnGN7QNJJQL/tJcDZwPmSBoGHqUIkIiJ6QJN7ENheCixtazuhZfop4LBRtnFiI8VFRMSIGg2IiWT6osu7XUJERE/p1W8xRURElyUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWAiIiImolICIiolYCIiIiaiUgIiKiVgIiIiJqJSAiIqJWowEhaa6kuyUNSlpUs3xrSReX5TdIml7a/5OkGyXdVm4ParLOiIhYX2MBIWkScCYwD5gFHC5pVlu3o4FHbO8OnA6cUtp/Dbzb9uuBI4Hzm6ozIiLqNbkHMQcYtL3c9jPARcD8tj7zgfPK9GLgYEmy/VPb95X2AWAbSVs3WGtERLRpMiCmACta5leWtto+ttcAjwE7t/X5Q+Am20+334GkYyT1S+ofGhoas8IjIqLHT1JL2pPqsNNH6pbbPsv2bNuz+/r6xre4iIjNXJMBsQqY1jI/tbTV9pE0GdgBeKjMTwUuBT5k++cN1hkRETWaDIhlwExJMyRtBSwElrT1WUJ1EhpgAXC1bUvaEbgcWGT7xw3WGBERw2gsIMo5hWOBK4A7gUtsD0g6SdKhpdvZwM6SBoG/ANZ9FfZYYHfgBEk3l79XNFVrRESsb3KTG7e9FFja1nZCy/RTwGE16/0N8DdN1hYRESPr6ZPUERHRPQmIiIiolYCIiIhaCYiIiKiVgIiIiFoJiIiIqJWAiIiIWgmIiIiolYCIiIhaCYiIiKiVgIiIiFoJiIiIqJWAiIiIWgmIiIiolYCIiIhaCYiIiKiVgIiIiFqNBoSkuZLuljQoaVHN8q0lXVyW3yBpesuyT5f2uyW9s8k6IyJifY0FhKRJwJnAPGAWcLikWW3djgYesb07cDpwSll3FrAQ2BOYC3ylbC8iIsZJk3sQc4BB28ttPwNcBMxv6zMfOK9MLwYOlqTSfpHtp23/Ahgs24uIiHEyucFtTwFWtMyvBPYbro/tNZIeA3Yu7de3rTul/Q4kHQMcU2ZXS7p7E+rdBfj1Jqw/niZSrTCx6p1ItcIo9eqUcaxkdJvVc9tLdMom1fqa4RY0GRCNs30WcNZYbEtSv+3ZY7Gtpk2kWmFi1TuRaoWJVe9EqhUmVr1N1drkIaZVwLSW+amlrbaPpMnADsBDHa4bERENajIglgEzJc2QtBXVSeclbX2WAEeW6QXA1bZd2heWbznNAGYCP2mw1oiIaNPYIaZyTuFY4ApgEnCO7QFJJwH9tpcAZwPnSxoEHqYKEUq/S4A7gDXAn9p+rqlaizE5VDVOJlKtMLHqnUi1wsSqdyLVChOr3kZqVfWBPSIi4oXyS+qIiKiVgIiIiFpbfEBI2lHSYkl3SbpT0gHdrmkkko6TNCDpdknfkvSSbte0jqRzJD0o6faWtpdLukrSz8rtTt2ssdUw9Z5a/i/cKulSSTt2scTn1dXasux4SZa0SzdqqzNcvZI+Xp7fAUlf6FZ9rYb5f7CPpOsl3SypX1JP/FBX0jRJP5R0R3kOP1HaG3mdbfEBAXwJ+J7t1wJ7A3d2uZ5hSZoC/Bkw2/brqE7+L+xuVS9wLtXQKK0WAT+wPRP4QZnvFeeyfr1XAa+zvRfwb8Cnx7uoYZzL+rUiaRrwDuDe8S5oFOfSVq+kt1ONkrC37T2BL3ahrjrnsv5z+wXgc7b3AU4o871gDXC87VnA/sCflqGJGnmdbdEBIWkH4ECqb1Nh+xnbj3a1qNFNBrYpvxvZFrivy/U8z/Y1VN9Ga9U6nMp5wHvGs6aR1NVr+0rba8rs9VS/wem6YZ5bqMYw+xTQU982GabejwIn23669Hlw3AurMUytBl5WpnegR15ntu+3fVOZfoLqA+0UGnqdbdEBAcwAhoCvS/qppK9J2q7bRQ3H9iqqT133AvcDj9m+srtVjeqVtu8v078CXtnNYjbQfwG+2+0ihiNpPrDK9i3drqVDewBvKSM3/z9J+3a7oBH8OXCqpBVUr7le2ZN8Xhn9+g3ADTT0OtvSA2Iy8EbgH22/AfgNvXUI5AXKccX5VMH2amA7SR/sblWdKz+C7KlPusOR9Fmq3flvdruWOpK2BT5DdfhjopgMvJzq0Mh/Ay4pg3P2oo8Cx9meBhxHOcrQKyRtD/wT8Oe2H29dNpavsy09IFYCK23fUOYXUwVGr/p94Be2h2w/C3wH+L0u1zSaByS9CqDc9sRhhZFIOgo4BDjCvftDod2oPijcIukeqkNhN0n6D12tamQrge+48hNgLdWAeL3oSKrXF8C36aHRpCW9mCocvml7XY2NvM626ICw/StghaTfLU0HU/16u1fdC+wvadvyyetgeviketE6nMqRwP/pYi2jkjSX6pj+obaf7HY9w7F9m+1X2J5uezrVm+8by//pXnUZ8HYASXsAW9G7o6XeB7y1TB8E/KyLtTyvvO7PBu60fVrLomZeZ7a36D9gH6AfuJXqP/BO3a5plHo/B9wF3A6cD2zd7ZpaavsW1bmRZ6nesI6mGr79B1QvsO8DL+92naPUO0g1BP3N5e9/dbvO4WptW34PsEu36xzlud0KuKD8370JOKjbdY5Q65uBG4FbqI7xv6nbdZZa30x1+OjWlv+jf9DU6yxDbURERK0t+hBTREQMLwERERG1EhAREVErAREREbUSEBERUSsBEbEBJD1XRvi8XdK3yy+ah+t7qKQRf5kvabqkD4x9pRGbLgERsWF+a3sfV6PpPgP8yXAdbS+xffIo25sOJCCiJyUgIjbetcDuZSz+y8o1JK6XtBdUQ3ZIOqNMnyvpy5L+VdJySQvKNk6mGsDu5nKtjz0l/aTM3yppZpceW0QCImJjlOHW5wG3Uf26/aeuriHxGeAbw6z2Kqpfwh5CFQxQDQ55bdkrOZ1qj+RLrq5DMJvql70RXTG52wVETDDbSLq5TF9LNS7ODcAfAti+WtLOkl5Ws+5lttcCd0gabjjm64DPSppKNbBdT4wBFFumBETEhvlt+XT/vA0Ysfrp1tXqOti+UNINwLuApZI+YvvqjSk0YlPlEFPEprsWOAJA0tuAX7ttjP4RPAG8dN2MpF2B5ba/TDUi515jWmnEBsgeRMSmOxE4R9KtwJP8+7DLnbgVeE7SLVTXRt4a+CNJz1JdGezvxrbUiM5lNNeIiKiVQ0wREVErAREREbUSEBERUSsBERERtRIQERFRKwERERG1EhAREVHr/wNk5NXuTsfc+gAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# converting column to list of points\n",
    "points_array = sorted([int(row.points) for row in df.select('points').collect()])\n",
    "  \n",
    "# Plotting the histogram.\n",
    "plt.hist(points_array, bins=5, density=True)\n",
    "plt.xlabel('Points')\n",
    "plt.ylabel('BinCounts')\n",
    "plt.title(r'Histogram of Points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exercise 2: Manipulating Recommender Dataset with Apache Spark\n",
    "<p><strong>For this exercise you will use movielens10m dataset available at https://grouplens.org/datasets/ movielens/10m/. The movielens dataset is a rating prediction dataset with ratings given on a scale of 1 to 5. Specifically, you will be working with Tags Data File Structure tags.dat, which contains data in the form \\UserID::MovieID::Tag::Timestamp\". You have to solve following questions using Apache Spark transformations and actions.</strong></p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+--------------------+----------+\n",
      "|UserID|MovieID|                 Tag| Timestamp|\n",
      "+------+-------+--------------------+----------+\n",
      "|    15|   4973|          excellent!|1215184630|\n",
      "|    20|   1747|            politics|1188263867|\n",
      "|    20|   1747|              satire|1188263867|\n",
      "|    20|   2424|     chick flick 212|1188263835|\n",
      "|    20|   2424|               hanks|1188263835|\n",
      "|    20|   2424|                ryan|1188263835|\n",
      "|    20|   2947|              action|1188263755|\n",
      "|    20|   2947|                bond|1188263756|\n",
      "|    20|   3033|               spoof|1188263880|\n",
      "|    20|   3033|           star wars|1188263880|\n",
      "|    20|   7438|              bloody|1188263801|\n",
      "|    20|   7438|             kung fu|1188263801|\n",
      "|    20|   7438|           Tarantino|1188263801|\n",
      "|    21|  55247|                   R|1205081506|\n",
      "|    21|  55253|               NC-17|1205081488|\n",
      "|    25|     50|        Kevin Spacey|1166101426|\n",
      "|    25|   6709|         Johnny Depp|1162147221|\n",
      "|    31|     65|        buddy comedy|1188263759|\n",
      "|    31|    546|strangely compelling|1188263674|\n",
      "|    31|   1091|         catastrophe|1188263741|\n",
      "+------+-------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "schema = StructType().add(\"UserID\",IntegerType(),True).add(\"MovieID\",IntegerType(),True).add(\"Tag\",StringType(),True).add(\"Timestamp\",IntegerType())\n",
    "tags=spark.read.options(delimiter=':').schema(schema).csv(\"tags.dat\")\n",
    "tags.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>1. A tagging session for a user can be defined as the duration in which he/she generated tagging activities. Typically, an inactive duration of 30 mins is considered as a termination of the tagging session. Your task is to separate out tagging sessions for each user.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+\n",
      "|UserID|      timestamp_list|\n",
      "+------+--------------------+\n",
      "|    15|        [1215184630]|\n",
      "|    20|[1188263755, 1188...|\n",
      "|    21|[1205081488, 1205...|\n",
      "|    25|[1162147221, 1166...|\n",
      "|    31|[1188263644, 1188...|\n",
      "|    32|        [1164735331]|\n",
      "|    39|[1188263764, 1188...|\n",
      "|    48|[1215135517, 1215...|\n",
      "|    49|[1188264095, 1188...|\n",
      "|    75|        [1162160415]|\n",
      "|    78|        [1176691425]|\n",
      "|   109|[1165554764, 1165...|\n",
      "|   127|[1188265347, 1188...|\n",
      "|   133|[1188265375, 1188...|\n",
      "|   146|[1147948639, 1148...|\n",
      "|   147|[1162188631, 1162...|\n",
      "|   170|        [1162209176]|\n",
      "|   175|[1188441420, 1192...|\n",
      "|   181|[1188266123, 1188...|\n",
      "|   190|[1140031954, 1140...|\n",
      "+------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a window for grouping user ids order by timestamp\n",
    "w = Window.partitionBy('UserID').orderBy('Timestamp')\n",
    "\n",
    "# creating new dataframe and adding userIds and corresponding timestamps in one row\n",
    "# collect_list is collecting timestamps of each user in the window, then grouped by userID to merge same userID rows\n",
    "session_list_df = tags.withColumn('timestamp_list', F.collect_list('Timestamp').over(w)).groupBy('UserID').agg(F.max('timestamp_list').alias('timestamp_list'))\n",
    "\n",
    "# ordering dataframe with userIds\n",
    "session_list_df = session_list_df.orderBy(F.col(\"UserID\").asc()).na.drop()\n",
    "session_list_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "|UserID|      timestamp_list|        session_list|\n",
      "+------+--------------------+--------------------+\n",
      "|    15|        [1215184630]|                 [1]|\n",
      "|    20|[1188263755, 1188...|[1, 1, 1, 1, 1, 1...|\n",
      "|    21|[1205081488, 1205...|              [1, 1]|\n",
      "|    25|[1162147221, 1166...|              [1, 2]|\n",
      "|    31|[1188263644, 1188...|     [1, 1, 1, 1, 1]|\n",
      "|    32|        [1164735331]|                 [1]|\n",
      "|    39|[1188263764, 1188...|     [1, 1, 1, 1, 1]|\n",
      "|    48|[1215135517, 1215...|              [1, 1]|\n",
      "|    49|[1188264095, 1188...|[1, 1, 1, 1, 1, 1...|\n",
      "|    75|        [1162160415]|                 [1]|\n",
      "|    78|        [1176691425]|                 [1]|\n",
      "|   109|[1165554764, 1165...|[1, 1, 1, 1, 1, 1...|\n",
      "|   127|[1188265347, 1188...|[1, 1, 1, 1, 1, 1...|\n",
      "|   133|[1188265375, 1188...|     [1, 1, 1, 1, 1]|\n",
      "|   146|[1147948639, 1148...|[1, 2, 3, 3, 4, 4...|\n",
      "|   147|[1162188631, 1162...|              [1, 1]|\n",
      "|   170|        [1162209176]|                 [1]|\n",
      "|   175|[1188441420, 1192...|              [1, 2]|\n",
      "|   181|[1188266123, 1188...|        [1, 1, 1, 1]|\n",
      "|   190|[1140031954, 1140...|[1, 1, 1, 2, 2, 2...|\n",
      "+------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating a udf which takes timestamp_list column and for each user , compare timestamp with previous one and assign session\n",
    "@udf(returnType=ArrayType(IntegerType()))\n",
    "def createSession(timeStampList):\n",
    "    session_list=[]\n",
    "    for i in range(len(timeStampList)):\n",
    "        if(i!=0):\n",
    "            # if timestamp difference is less than 1800 seconds then session is not changed\n",
    "            if((timeStampList[i]-timeStampList[i-1]) < 1800):\n",
    "                session=previous_session\n",
    "\n",
    "            else:\n",
    "                session=previous_session+1\n",
    "        else:\n",
    "            session=1\n",
    "        session_list.append(session)\n",
    "        previous_session=session\n",
    "            \n",
    "\n",
    "    return session_list\n",
    "#calling udf for adding session_list column which has session ids for each tag\n",
    "session_list_df=session_list_df.withColumn(\"session_list\",createSession(session_list_df.timestamp_list))\n",
    "session_list_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 3, 3, 3, 4, 5, 6, 7, 7, 7, 7, 8, 9]"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing sessoion list of tags for userID 109\n",
    "session_list_df.filter(session_list_df.UserID==109).collect()[0]['session_list']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### <strong>2. Once you have all the tagging sessions for each user, calculate the frequency of tagging for each user session</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "|UserID|      timestamp_list|        session_list|frequence_of_tagging|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|    15|        [1215184630]|                 [1]|                 [1]|\n",
      "|    20|[1188263755, 1188...|[1, 1, 1, 1, 1, 1...|                [12]|\n",
      "|    21|[1205081488, 1205...|              [1, 1]|                 [2]|\n",
      "|    25|[1162147221, 1166...|              [1, 2]|              [1, 1]|\n",
      "|    31|[1188263644, 1188...|     [1, 1, 1, 1, 1]|                 [5]|\n",
      "|    32|        [1164735331]|                 [1]|                 [1]|\n",
      "|    39|[1188263764, 1188...|     [1, 1, 1, 1, 1]|                 [5]|\n",
      "|    48|[1215135517, 1215...|              [1, 1]|                 [2]|\n",
      "|    49|[1188264095, 1188...|[1, 1, 1, 1, 1, 1...|                [15]|\n",
      "|    75|        [1162160415]|                 [1]|                 [1]|\n",
      "|    78|        [1176691425]|                 [1]|                 [1]|\n",
      "|   109|[1165554764, 1165...|[1, 1, 1, 1, 1, 1...|[11, 2, 3, 1, 1, ...|\n",
      "|   127|[1188265347, 1188...|[1, 1, 1, 1, 1, 1...|                [26]|\n",
      "|   133|[1188265375, 1188...|     [1, 1, 1, 1, 1]|                 [5]|\n",
      "|   146|[1147948639, 1148...|[1, 2, 3, 3, 4, 4...|[1, 1, 2, 12, 2, ...|\n",
      "|   147|[1162188631, 1162...|              [1, 1]|                 [2]|\n",
      "|   170|        [1162209176]|                 [1]|                 [1]|\n",
      "|   175|[1188441420, 1192...|              [1, 2]|              [1, 1]|\n",
      "|   181|[1188266123, 1188...|        [1, 1, 1, 1]|                 [4]|\n",
      "|   190|[1140031954, 1140...|[1, 1, 1, 2, 2, 2...|       [3, 3, 17, 3]|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating udf for calculating frequency from session list\n",
    "@udf(returnType=ArrayType(IntegerType()))\n",
    "def CountFrequency(sessionList):\n",
    "    # count each session id in list\n",
    "    C = Counter(sessionList)\n",
    "    return list(C.values())\n",
    "\n",
    "# calling udf and creating new column frequence_of_tagging\n",
    "session_list_df=session_list_df.withColumn(\"frequence_of_tagging\",CountFrequency(session_list_df.session_list))\n",
    "session_list_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 2, 3, 1, 1, 1, 4, 1, 1]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing frequency list of tags for each session for userID 109\n",
    "session_list_df.filter(session_list_df.UserID==109).collect()[0]['frequence_of_tagging']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>3. Find a mean and standard deviation of the tagging frequency of each user.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+------------------------+-----------------------+\n",
      "|UserID|frequence_of_tagging|mean_Frequency_each_user|std_Frequency_each_user|\n",
      "+------+--------------------+------------------------+-----------------------+\n",
      "|    15|                 [1]|                     1.0|                    0.0|\n",
      "|    20|                [12]|                    12.0|                    0.0|\n",
      "|    21|                 [2]|                     2.0|                    0.0|\n",
      "|    25|              [1, 1]|                     1.0|                    0.0|\n",
      "|    31|                 [5]|                     5.0|                    0.0|\n",
      "|    32|                 [1]|                     1.0|                    0.0|\n",
      "|    39|                 [5]|                     5.0|                    0.0|\n",
      "|    48|                 [2]|                     2.0|                    0.0|\n",
      "|    49|                [15]|                    15.0|                    0.0|\n",
      "|    75|                 [1]|                     1.0|                    0.0|\n",
      "|    78|                 [1]|                     1.0|                    0.0|\n",
      "|   109|[11, 2, 3, 1, 1, ...|               2.7777777|              3.2702363|\n",
      "|   127|                [26]|                    26.0|                    0.0|\n",
      "|   133|                 [5]|                     5.0|                    0.0|\n",
      "|   146|[1, 1, 2, 12, 2, ...|                4.948949|              9.0100565|\n",
      "|   147|                 [2]|                     2.0|                    0.0|\n",
      "|   170|                 [1]|                     1.0|                    0.0|\n",
      "|   175|              [1, 1]|                     1.0|                    0.0|\n",
      "|   181|                 [4]|                     4.0|                    0.0|\n",
      "|   190|       [3, 3, 17, 3]|                     6.5|                    7.0|\n",
      "+------+--------------------+------------------------+-----------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# creating udf for calculating mean frequency tagging for each session of user\n",
    "@udf(FloatType())\n",
    "def CalculateMean(freqList):\n",
    "    sum= 0\n",
    "    for i in range(len(freqList)):\n",
    "        sum += freqList[i]\n",
    "    return sum/(len(freqList))\n",
    "\n",
    "session_list_df=session_list_df.withColumn(\"mean_Frequency_each_user\",CalculateMean(session_list_df.frequence_of_tagging))\n",
    "\n",
    "# creating udf for calculating std of frequency tagging for each session of user\n",
    "@udf(FloatType())\n",
    "def CalculateStd(freqList):\n",
    "    std = 0.0\n",
    "    if(len(freqList)>1):\n",
    "        std = statistics.stdev(freqList)\n",
    "    return std\n",
    "\n",
    "session_list_df=session_list_df.withColumn(\"std_Frequency_each_user\",CalculateStd(session_list_df.frequence_of_tagging))\n",
    "session_list_df.select(['UserID','frequence_of_tagging','mean_Frequency_each_user','std_Frequency_each_user']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.777777671813965"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing mean of frequency list of tags for each session for userID 109\n",
    "session_list_df.filter(session_list_df.UserID==109).collect()[0]['mean_Frequency_each_user']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.2702362537384033"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# showing  std of frequency list of tags for each session for userID 109\n",
    "session_list_df.filter(session_list_df.UserID==109).collect()[0]['std_Frequency_each_user']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong>4. Find a mean and standard deviation of the tagging frequency for across users.</strong></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting frequence_of_tagging to 2d list and then converting it to ID\n",
    "frequency_list = list(chain.from_iterable(session_list_df.select('frequence_of_tagging').rdd.flatMap(lambda x: x).collect()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------------------+----------------+-----------------+\n",
      "|UserID|mean_Frequency_each_user|std_Frequency_each_user|mean_across_user|  std_across_user|\n",
      "+------+------------------------+-----------------------+----------------+-----------------+\n",
      "|    15|                     1.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    20|                    12.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    21|                     2.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    25|                     1.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    31|                     5.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    32|                     1.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    39|                     5.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    48|                     2.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    49|                    15.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    75|                     1.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|    78|                     1.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|   109|               2.7777777|              3.2702363|7.17114764172684|22.07944597647594|\n",
      "|   127|                    26.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|   133|                     5.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|   146|                4.948949|              9.0100565|7.17114764172684|22.07944597647594|\n",
      "|   147|                     2.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|   170|                     1.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|   175|                     1.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|   181|                     4.0|                    0.0|7.17114764172684|22.07944597647594|\n",
      "|   190|                     6.5|                    7.0|7.17114764172684|22.07944597647594|\n",
      "+------+------------------------+-----------------------+----------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# calculating std and adding in a column\n",
    "stdPoints = statistics.stdev(frequency_list)\n",
    "session_list_df = session_list_df.withColumn(\"std_across_user\",F.lit(stdPoints))\n",
    "\n",
    "# calculating mean and adding in a column\n",
    "meanPoints = statistics.mean(frequency_list)\n",
    "session_list_df = session_list_df.withColumn(\"mean_across_user\",F.lit(meanPoints))\n",
    "session_list_df.select(['UserID','mean_Frequency_each_user','std_Frequency_each_user','mean_across_user','std_across_user']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Std accross user: 22.07944597647594\n",
      "Mean accross user: 7.17114764172684\n"
     ]
    }
   ],
   "source": [
    "print('Std accross user:',statistics.stdev(frequency_list))\n",
    "print('Mean accross user:',statistics.mean(frequency_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <strong> 5. Provide the list of users with a mean tagging frequency within the two standard deviation from the mean frequency of all users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------------+----------------+--------------------+\n",
      "|UserID|mean_Frequency_each_user|  std_across_user|mean_across_user|is_two_std_from_mean|\n",
      "+------+------------------------+-----------------+----------------+--------------------+\n",
      "|    15|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    20|                    12.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    21|                     2.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    25|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    31|                     5.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    32|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    39|                     5.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    48|                     2.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    49|                    15.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    75|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    78|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   109|               2.7777777|22.07944597647594|7.17114764172684|                   1|\n",
      "|   127|                    26.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   133|                     5.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   146|                4.948949|22.07944597647594|7.17114764172684|                   1|\n",
      "|   147|                     2.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   170|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   175|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   181|                     4.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   190|                     6.5|22.07944597647594|7.17114764172684|                   1|\n",
      "+------+------------------------+-----------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# defining udf which puts 1 in is_two_std_from_mean column if users mean frequency is under 2*StandarDeviation else 0\n",
    "@udf(\"int\")\n",
    "def checkStdFromMean(freq,std,mean):\n",
    "    if mean - 2 * std < freq < mean + 2 * std:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "session_list_df=session_list_df.withColumn(\"is_two_std_from_mean\",checkStdFromMean(session_list_df.mean_Frequency_each_user,session_list_df.std_across_user,session_list_df.mean_across_user))\n",
    "session_list_df.select(['UserID','mean_Frequency_each_user','std_across_user','mean_across_user','is_two_std_from_mean']).show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------+-----------------+----------------+--------------------+\n",
      "|UserID|mean_Frequency_each_user|  std_across_user|mean_across_user|is_two_std_from_mean|\n",
      "+------+------------------------+-----------------+----------------+--------------------+\n",
      "|    15|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    20|                    12.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    21|                     2.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    25|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    31|                     5.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    32|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    39|                     5.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    48|                     2.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    49|                    15.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    75|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|    78|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   109|               2.7777777|22.07944597647594|7.17114764172684|                   1|\n",
      "|   127|                    26.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   133|                     5.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   146|                4.948949|22.07944597647594|7.17114764172684|                   1|\n",
      "|   147|                     2.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   170|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   175|                     1.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   181|                     4.0|22.07944597647594|7.17114764172684|                   1|\n",
      "|   190|                     6.5|22.07944597647594|7.17114764172684|                   1|\n",
      "+------+------------------------+-----------------+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# showing users within 2 std from mean frequency\n",
    "session_list_df.filter(session_list_df.is_two_std_from_mean==1).select(['UserID','mean_Frequency_each_user','std_across_user','mean_across_user','is_two_std_from_mean']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[15,\n",
       " 20,\n",
       " 21,\n",
       " 25,\n",
       " 31,\n",
       " 32,\n",
       " 39,\n",
       " 48,\n",
       " 49,\n",
       " 75,\n",
       " 78,\n",
       " 109,\n",
       " 127,\n",
       " 133,\n",
       " 146,\n",
       " 147,\n",
       " 170,\n",
       " 175,\n",
       " 181,\n",
       " 190,\n",
       " 222,\n",
       " 233,\n",
       " 240,\n",
       " 249,\n",
       " 267,\n",
       " 283,\n",
       " 284,\n",
       " 299,\n",
       " 325,\n",
       " 374,\n",
       " 383,\n",
       " 423,\n",
       " 442,\n",
       " 457,\n",
       " 476,\n",
       " 477,\n",
       " 487,\n",
       " 493,\n",
       " 498,\n",
       " 514,\n",
       " 519,\n",
       " 528,\n",
       " 533,\n",
       " 538,\n",
       " 545,\n",
       " 548,\n",
       " 580,\n",
       " 611,\n",
       " 615,\n",
       " 622,\n",
       " 626,\n",
       " 636,\n",
       " 637,\n",
       " 673,\n",
       " 682,\n",
       " 687,\n",
       " 692,\n",
       " 713,\n",
       " 717,\n",
       " 722,\n",
       " 728,\n",
       " 732,\n",
       " 760,\n",
       " 778,\n",
       " 783,\n",
       " 788,\n",
       " 845,\n",
       " 850,\n",
       " 858,\n",
       " 877,\n",
       " 880,\n",
       " 894,\n",
       " 922,\n",
       " 929,\n",
       " 937,\n",
       " 944,\n",
       " 959,\n",
       " 981,\n",
       " 985,\n",
       " 1000,\n",
       " 1010,\n",
       " 1017,\n",
       " 1035,\n",
       " 1036,\n",
       " 1042,\n",
       " 1047,\n",
       " 1049,\n",
       " 1056,\n",
       " 1081,\n",
       " 1117,\n",
       " 1118,\n",
       " 1122,\n",
       " 1137,\n",
       " 1160,\n",
       " 1164,\n",
       " 1170,\n",
       " 1174,\n",
       " 1188,\n",
       " 1198,\n",
       " 1210,\n",
       " 1224,\n",
       " 1225,\n",
       " 1241,\n",
       " 1246,\n",
       " 1267,\n",
       " 1272,\n",
       " 1278,\n",
       " 1282,\n",
       " 1295,\n",
       " 1298,\n",
       " 1338,\n",
       " 1355,\n",
       " 1356,\n",
       " 1362,\n",
       " 1379,\n",
       " 1392,\n",
       " 1393,\n",
       " 1436,\n",
       " 1443,\n",
       " 1447,\n",
       " 1457,\n",
       " 1496,\n",
       " 1504,\n",
       " 1510,\n",
       " 1539,\n",
       " 1547,\n",
       " 1553,\n",
       " 1565,\n",
       " 1575,\n",
       " 1600,\n",
       " 1613,\n",
       " 1622,\n",
       " 1632,\n",
       " 1652,\n",
       " 1663,\n",
       " 1670,\n",
       " 1682,\n",
       " 1698,\n",
       " 1713,\n",
       " 1724,\n",
       " 1728,\n",
       " 1751,\n",
       " 1757,\n",
       " 1778,\n",
       " 1806,\n",
       " 1858,\n",
       " 1860,\n",
       " 1894,\n",
       " 1910,\n",
       " 1915,\n",
       " 1920,\n",
       " 1946,\n",
       " 1967,\n",
       " 1972,\n",
       " 1988,\n",
       " 2000,\n",
       " 2014,\n",
       " 2024,\n",
       " 2039,\n",
       " 2040,\n",
       " 2053,\n",
       " 2075,\n",
       " 2098,\n",
       " 2102,\n",
       " 2108,\n",
       " 2136,\n",
       " 2158,\n",
       " 2175,\n",
       " 2200,\n",
       " 2204,\n",
       " 2206,\n",
       " 2212,\n",
       " 2218,\n",
       " 2276,\n",
       " 2315,\n",
       " 2316,\n",
       " 2346,\n",
       " 2429,\n",
       " 2435,\n",
       " 2455,\n",
       " 2456,\n",
       " 2471,\n",
       " 2477,\n",
       " 2513,\n",
       " 2514,\n",
       " 2517,\n",
       " 2532,\n",
       " 2536,\n",
       " 2546,\n",
       " 2556,\n",
       " 2558,\n",
       " 2571,\n",
       " 2589,\n",
       " 2606,\n",
       " 2612,\n",
       " 2639,\n",
       " 2643,\n",
       " 2644,\n",
       " 2645,\n",
       " 2658,\n",
       " 2687,\n",
       " 2692,\n",
       " 2693,\n",
       " 2706,\n",
       " 2732,\n",
       " 2743,\n",
       " 2761,\n",
       " 2766,\n",
       " 2771,\n",
       " 2778,\n",
       " 2786,\n",
       " 2801,\n",
       " 2842,\n",
       " 2852,\n",
       " 2853,\n",
       " 2854,\n",
       " 2891,\n",
       " 2901,\n",
       " 2904,\n",
       " 2930,\n",
       " 2953,\n",
       " 2978,\n",
       " 2985,\n",
       " 3017,\n",
       " 3028,\n",
       " 3039,\n",
       " 3057,\n",
       " 3058,\n",
       " 3104,\n",
       " 3106,\n",
       " 3174,\n",
       " 3190,\n",
       " 3192,\n",
       " 3193,\n",
       " 3205,\n",
       " 3224,\n",
       " 3251,\n",
       " 3271,\n",
       " 3301,\n",
       " 3304,\n",
       " 3307,\n",
       " 3313,\n",
       " 3320,\n",
       " 3331,\n",
       " 3369,\n",
       " 3386,\n",
       " 3392,\n",
       " 3419,\n",
       " 3429,\n",
       " 3478,\n",
       " 3489,\n",
       " 3490,\n",
       " 3493,\n",
       " 3512,\n",
       " 3527,\n",
       " 3541,\n",
       " 3595,\n",
       " 3600,\n",
       " 3622,\n",
       " 3630,\n",
       " 3710,\n",
       " 3743,\n",
       " 3785,\n",
       " 3825,\n",
       " 3844,\n",
       " 3849,\n",
       " 3853,\n",
       " 3860,\n",
       " 3868,\n",
       " 3911,\n",
       " 3948,\n",
       " 3962,\n",
       " 3976,\n",
       " 3999,\n",
       " 4002,\n",
       " 4003,\n",
       " 4011,\n",
       " 4022,\n",
       " 4078,\n",
       " 4087,\n",
       " 4137,\n",
       " 4146,\n",
       " 4188,\n",
       " 4198,\n",
       " 4254,\n",
       " 4291,\n",
       " 4335,\n",
       " 4341,\n",
       " 4352,\n",
       " 4356,\n",
       " 4375,\n",
       " 4397,\n",
       " 4517,\n",
       " 4544,\n",
       " 4549,\n",
       " 4575,\n",
       " 4591,\n",
       " 4607,\n",
       " 4650,\n",
       " 4662,\n",
       " 4668,\n",
       " 4677,\n",
       " 4682,\n",
       " 4697,\n",
       " 4728,\n",
       " 4737,\n",
       " 4800,\n",
       " 4827,\n",
       " 4856,\n",
       " 4869,\n",
       " 4895,\n",
       " 4905,\n",
       " 4912,\n",
       " 4913,\n",
       " 4944,\n",
       " 4957,\n",
       " 4991,\n",
       " 4998,\n",
       " 5002,\n",
       " 5008,\n",
       " 5020,\n",
       " 5031,\n",
       " 5059,\n",
       " 5063,\n",
       " 5076,\n",
       " 5085,\n",
       " 5161,\n",
       " 5166,\n",
       " 5182,\n",
       " 5210,\n",
       " 5223,\n",
       " 5227,\n",
       " 5228,\n",
       " 5238,\n",
       " 5240,\n",
       " 5285,\n",
       " 5321,\n",
       " 5325,\n",
       " 5459,\n",
       " 5461,\n",
       " 5462,\n",
       " 5475,\n",
       " 5494,\n",
       " 5552,\n",
       " 5583,\n",
       " 5593,\n",
       " 5621,\n",
       " 5706,\n",
       " 5739,\n",
       " 5757,\n",
       " 5829,\n",
       " 5831,\n",
       " 5844,\n",
       " 5859,\n",
       " 5899,\n",
       " 5917,\n",
       " 5925,\n",
       " 5936,\n",
       " 5954,\n",
       " 5956,\n",
       " 5978,\n",
       " 6018,\n",
       " 6038,\n",
       " 6052,\n",
       " 6058,\n",
       " 6068,\n",
       " 6082,\n",
       " 6119,\n",
       " 6132,\n",
       " 6150,\n",
       " 6169,\n",
       " 6189,\n",
       " 6224,\n",
       " 6227,\n",
       " 6264,\n",
       " 6289,\n",
       " 6302,\n",
       " 6311,\n",
       " 6323,\n",
       " 6348,\n",
       " 6352,\n",
       " 6362,\n",
       " 6375,\n",
       " 6385,\n",
       " 6393,\n",
       " 6400,\n",
       " 6443,\n",
       " 6449,\n",
       " 6471,\n",
       " 6477,\n",
       " 6482,\n",
       " 6496,\n",
       " 6504,\n",
       " 6528,\n",
       " 6532,\n",
       " 6550,\n",
       " 6571,\n",
       " 6616,\n",
       " 6625,\n",
       " 6658,\n",
       " 6722,\n",
       " 6750,\n",
       " 6757,\n",
       " 6758,\n",
       " 6785,\n",
       " 6862,\n",
       " 6891,\n",
       " 6905,\n",
       " 6927,\n",
       " 6946,\n",
       " 6971,\n",
       " 7008,\n",
       " 7088,\n",
       " 7095,\n",
       " 7109,\n",
       " 7127,\n",
       " 7172,\n",
       " 7181,\n",
       " 7195,\n",
       " 7248,\n",
       " 7252,\n",
       " 7287,\n",
       " 7315,\n",
       " 7317,\n",
       " 7354,\n",
       " 7357,\n",
       " 7369,\n",
       " 7373,\n",
       " 7461,\n",
       " 7477,\n",
       " 7530,\n",
       " 7536,\n",
       " 7538,\n",
       " 7551,\n",
       " 7587,\n",
       " 7612,\n",
       " 7625,\n",
       " 7635,\n",
       " 7637,\n",
       " 7685,\n",
       " 7695,\n",
       " 7704,\n",
       " 7711,\n",
       " 7725,\n",
       " 7727,\n",
       " 7740,\n",
       " 7743,\n",
       " 7815,\n",
       " 7849,\n",
       " 7871,\n",
       " 7881,\n",
       " 7885,\n",
       " 7919,\n",
       " 7939,\n",
       " 7944,\n",
       " 8030,\n",
       " 8041,\n",
       " 8048,\n",
       " 8051,\n",
       " 8094,\n",
       " 8114,\n",
       " 8127,\n",
       " 8128,\n",
       " 8148,\n",
       " 8168,\n",
       " 8194,\n",
       " 8209,\n",
       " 8216,\n",
       " 8217,\n",
       " 8282,\n",
       " 8286,\n",
       " 8296,\n",
       " 8302,\n",
       " 8321,\n",
       " 8381,\n",
       " 8409,\n",
       " 8423,\n",
       " 8434,\n",
       " 8463,\n",
       " 8477,\n",
       " 8517,\n",
       " 8531,\n",
       " 8545,\n",
       " 8584,\n",
       " 8585,\n",
       " 8589,\n",
       " 8604,\n",
       " 8610,\n",
       " 8619,\n",
       " 8621,\n",
       " 8624,\n",
       " 8642,\n",
       " 8651,\n",
       " 8669,\n",
       " 8683,\n",
       " 8699,\n",
       " 8701,\n",
       " 8702,\n",
       " 8707,\n",
       " 8726,\n",
       " 8727,\n",
       " 8740,\n",
       " 8758,\n",
       " 8766,\n",
       " 8777,\n",
       " 8787,\n",
       " 8824,\n",
       " 8858,\n",
       " 8881,\n",
       " 8892,\n",
       " 8893,\n",
       " 8905,\n",
       " 8930,\n",
       " 8937,\n",
       " 8970,\n",
       " 8996,\n",
       " 8997,\n",
       " 9006,\n",
       " 9011,\n",
       " 9017,\n",
       " 9029,\n",
       " 9030,\n",
       " 9034,\n",
       " 9057,\n",
       " 9060,\n",
       " 9077,\n",
       " 9085,\n",
       " 9117,\n",
       " 9127,\n",
       " 9134,\n",
       " 9139,\n",
       " 9177,\n",
       " 9198,\n",
       " 9245,\n",
       " 9264,\n",
       " 9277,\n",
       " 9283,\n",
       " 9288,\n",
       " 9292,\n",
       " 9316,\n",
       " 9317,\n",
       " 9370,\n",
       " 9411,\n",
       " 9415,\n",
       " 9416,\n",
       " 9426,\n",
       " 9463,\n",
       " 9469,\n",
       " 9470,\n",
       " 9475,\n",
       " 9482,\n",
       " 9522,\n",
       " 9525,\n",
       " 9529,\n",
       " 9593,\n",
       " 9595,\n",
       " 9688,\n",
       " 9733,\n",
       " 9737,\n",
       " 9792,\n",
       " 9799,\n",
       " 9807,\n",
       " 9811,\n",
       " 9845,\n",
       " 9889,\n",
       " 9946,\n",
       " 9965,\n",
       " 9970,\n",
       " 9976,\n",
       " 9988,\n",
       " 10003,\n",
       " 10020,\n",
       " 10025,\n",
       " 10032,\n",
       " 10051,\n",
       " 10058,\n",
       " 10059,\n",
       " 10064,\n",
       " 10084,\n",
       " 10094,\n",
       " 10117,\n",
       " 10125,\n",
       " 10132,\n",
       " 10154,\n",
       " 10167,\n",
       " 10181,\n",
       " 10191,\n",
       " 10200,\n",
       " 10215,\n",
       " 10221,\n",
       " 10231,\n",
       " 10232,\n",
       " 10272,\n",
       " 10308,\n",
       " 10309,\n",
       " 10319,\n",
       " 10329,\n",
       " 10335,\n",
       " 10356,\n",
       " 10402,\n",
       " 10420,\n",
       " 10439,\n",
       " 10471,\n",
       " 10476,\n",
       " 10479,\n",
       " 10506,\n",
       " 10516,\n",
       " 10521,\n",
       " 10523,\n",
       " 10555,\n",
       " 10563,\n",
       " 10564,\n",
       " 10608,\n",
       " 10639,\n",
       " 10644,\n",
       " 10652,\n",
       " 10674,\n",
       " 10683,\n",
       " 10739,\n",
       " 10768,\n",
       " 10783,\n",
       " 10788,\n",
       " 10791,\n",
       " 10817,\n",
       " 10827,\n",
       " 10844,\n",
       " 10885,\n",
       " 10894,\n",
       " 10905,\n",
       " 10913,\n",
       " 10926,\n",
       " 10931,\n",
       " 10936,\n",
       " 10986,\n",
       " 11032,\n",
       " 11043,\n",
       " 11050,\n",
       " 11057,\n",
       " 11090,\n",
       " 11104,\n",
       " 11149,\n",
       " 11162,\n",
       " 11188,\n",
       " 11220,\n",
       " 11228,\n",
       " 11229,\n",
       " 11239,\n",
       " 11240,\n",
       " 11244,\n",
       " 11245,\n",
       " 11255,\n",
       " 11258,\n",
       " 11271,\n",
       " 11277,\n",
       " 11335,\n",
       " 11338,\n",
       " 11368,\n",
       " 11411,\n",
       " 11420,\n",
       " 11421,\n",
       " 11424,\n",
       " 11425,\n",
       " 11451,\n",
       " 11457,\n",
       " 11472,\n",
       " 11486,\n",
       " 11510,\n",
       " 11511,\n",
       " 11515,\n",
       " 11548,\n",
       " 11554,\n",
       " 11555,\n",
       " 11563,\n",
       " 11569,\n",
       " 11577,\n",
       " 11608,\n",
       " 11613,\n",
       " 11638,\n",
       " 11650,\n",
       " 11653,\n",
       " 11705,\n",
       " 11728,\n",
       " 11744,\n",
       " 11758,\n",
       " 11760,\n",
       " 11802,\n",
       " 11809,\n",
       " 11810,\n",
       " 11824,\n",
       " 11825,\n",
       " 11898,\n",
       " 11912,\n",
       " 11930,\n",
       " 11937,\n",
       " 12028,\n",
       " 12040,\n",
       " 12046,\n",
       " 12051,\n",
       " 12056,\n",
       " 12057,\n",
       " 12064,\n",
       " 12088,\n",
       " 12100,\n",
       " 12124,\n",
       " 12126,\n",
       " 12221,\n",
       " 12227,\n",
       " 12259,\n",
       " 12265,\n",
       " 12267,\n",
       " 12272,\n",
       " 12273,\n",
       " 12283,\n",
       " 12296,\n",
       " 12299,\n",
       " 12309,\n",
       " 12313,\n",
       " 12334,\n",
       " 12356,\n",
       " 12383,\n",
       " 12391,\n",
       " 12428,\n",
       " 12429,\n",
       " 12434,\n",
       " 12451,\n",
       " 12458,\n",
       " 12459,\n",
       " 12468,\n",
       " 12490,\n",
       " 12526,\n",
       " 12543,\n",
       " 12554,\n",
       " 12563,\n",
       " 12564,\n",
       " 12601,\n",
       " 12621,\n",
       " 12622,\n",
       " 12626,\n",
       " 12681,\n",
       " 12691,\n",
       " 12759,\n",
       " 12760,\n",
       " 12764,\n",
       " 12818,\n",
       " 12866,\n",
       " 12888,\n",
       " 12890,\n",
       " 12911,\n",
       " 12917,\n",
       " 12921,\n",
       " 12941,\n",
       " 12949,\n",
       " 12950,\n",
       " 12986,\n",
       " 13005,\n",
       " 13014,\n",
       " 13029,\n",
       " 13043,\n",
       " 13048,\n",
       " 13049,\n",
       " 13054,\n",
       " 13056,\n",
       " 13086,\n",
       " 13100,\n",
       " 13151,\n",
       " 13167,\n",
       " 13216,\n",
       " 13222,\n",
       " 13230,\n",
       " 13233,\n",
       " 13257,\n",
       " 13266,\n",
       " 13283,\n",
       " 13296,\n",
       " 13318,\n",
       " 13346,\n",
       " 13376,\n",
       " 13396,\n",
       " 13397,\n",
       " 13403,\n",
       " 13436,\n",
       " 13454,\n",
       " 13462,\n",
       " 13469,\n",
       " 13472,\n",
       " 13480,\n",
       " 13515,\n",
       " 13531,\n",
       " 13537,\n",
       " 13561,\n",
       " 13595,\n",
       " 13614,\n",
       " 13656,\n",
       " 13688,\n",
       " 13694,\n",
       " 13695,\n",
       " 13701,\n",
       " 13716,\n",
       " 13718,\n",
       " 13726,\n",
       " 13738,\n",
       " 13748,\n",
       " 13781,\n",
       " 13815,\n",
       " 13821,\n",
       " 13853,\n",
       " 13870,\n",
       " 13877,\n",
       " 13880,\n",
       " 13890,\n",
       " 13897,\n",
       " 13914,\n",
       " 13916,\n",
       " 13937,\n",
       " 13938,\n",
       " 13970,\n",
       " 14029,\n",
       " 14036,\n",
       " 14048,\n",
       " 14070,\n",
       " 14095,\n",
       " 14099,\n",
       " 14141,\n",
       " 14144,\n",
       " 14149,\n",
       " 14160,\n",
       " 14171,\n",
       " 14208,\n",
       " 14222,\n",
       " 14239,\n",
       " 14246,\n",
       " 14280,\n",
       " 14292,\n",
       " 14323,\n",
       " 14343,\n",
       " 14361,\n",
       " 14378,\n",
       " 14421,\n",
       " 14423,\n",
       " 14454,\n",
       " 14465,\n",
       " 14479,\n",
       " 14498,\n",
       " 14531,\n",
       " 14557,\n",
       " 14570,\n",
       " 14578,\n",
       " 14598,\n",
       " 14599,\n",
       " 14614,\n",
       " 14648,\n",
       " 14653,\n",
       " 14660,\n",
       " 14679,\n",
       " 14681,\n",
       " 14706,\n",
       " 14709,\n",
       " 14775,\n",
       " 14844,\n",
       " 14888,\n",
       " 14895,\n",
       " 14910,\n",
       " 14911,\n",
       " 14918,\n",
       " 14924,\n",
       " 14946,\n",
       " 14953,\n",
       " 14973,\n",
       " 14979,\n",
       " 14984,\n",
       " 15017,\n",
       " 15037,\n",
       " 15044,\n",
       " 15061,\n",
       " 15087,\n",
       " 15096,\n",
       " 15105,\n",
       " 15111,\n",
       " 15157,\n",
       " 15168,\n",
       " 15178,\n",
       " 15250,\n",
       " 15264,\n",
       " 15286,\n",
       " 15311,\n",
       " 15329,\n",
       " 15336,\n",
       " 15344,\n",
       " 15358,\n",
       " 15385,\n",
       " 15399,\n",
       " 15436,\n",
       " 15437,\n",
       " 15440,\n",
       " 15449,\n",
       " 15558,\n",
       " 15626,\n",
       " 15647,\n",
       " 15650,\n",
       " 15663,\n",
       " 15702,\n",
       " 15707,\n",
       " 15714,\n",
       " 15715,\n",
       " 15716,\n",
       " 15722,\n",
       " 15745,\n",
       " 15751,\n",
       " 15780,\n",
       " 15783,\n",
       " 15789,\n",
       " 15793,\n",
       " 15804,\n",
       " 15811,\n",
       " 15816,\n",
       " 15817,\n",
       " 15821,\n",
       " 15844,\n",
       " 15846,\n",
       " 15851,\n",
       " 15853,\n",
       " 15879,\n",
       " 15899,\n",
       " 15926,\n",
       " 15937,\n",
       " 15959,\n",
       " 15965,\n",
       " 15974,\n",
       " 15987,\n",
       " 15992,\n",
       " 15996,\n",
       " 16003,\n",
       " 16004,\n",
       " 16031,\n",
       " 16039,\n",
       " 16047,\n",
       " 16048,\n",
       " 16094,\n",
       " 16098,\n",
       " 16103,\n",
       " 16108,\n",
       " 16113,\n",
       " 16123,\n",
       " 16133,\n",
       " 16140,\n",
       " 16149,\n",
       " 16155,\n",
       " 16168,\n",
       " 16174,\n",
       " 16206,\n",
       " 16207,\n",
       " 16234,\n",
       " 16235,\n",
       " 16295,\n",
       " 16302,\n",
       " 16321,\n",
       " 16322,\n",
       " 16349,\n",
       " 16389,\n",
       " 16419,\n",
       " 16425,\n",
       " 16428,\n",
       " 16462,\n",
       " 16485,\n",
       " 16565,\n",
       " 16574,\n",
       " 16587,\n",
       " 16591,\n",
       " 16592,\n",
       " 16610,\n",
       " 16664,\n",
       " 16677,\n",
       " 16751,\n",
       " 16754,\n",
       " 16758,\n",
       " 16766,\n",
       " 16779,\n",
       " 16836,\n",
       " 16850,\n",
       " 16862,\n",
       " 16890,\n",
       " 16949,\n",
       " 16972,\n",
       " 16983,\n",
       " 16987,\n",
       " 17054,\n",
       " 17074,\n",
       " 17094,\n",
       " 17205,\n",
       " 17215,\n",
       " 17239,\n",
       " 17249,\n",
       " 17260,\n",
       " 17266,\n",
       " 17298,\n",
       " 17307,\n",
       " 17312,\n",
       " 17326,\n",
       " 17334,\n",
       " 17363,\n",
       " ...]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_List_with_2_std = sorted([int(row.UserID) for row in session_list_df.filter(session_list_df.is_two_std_from_mean==1).collect()])\n",
    "user_List_with_2_std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "https://stackoverflow.com/questions/28240706/explain-the-aggregate-functionality-in-spark-with-python-and-scala"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
